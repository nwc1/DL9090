import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Dense, Dropout

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize and flatten the images
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

X_train = X_train.reshape((len(X_train), -1))  # Flatten to (60000, 784)
X_test = X_test.reshape((len(X_test), -1))     # Flatten to (10000, 784)

# Define the size of the encoding
encoding_dim = 64  # 64-dimensional latent space

# ==========================
# Build and Train Autoencoder
# ==========================

# Input layer
input_img = Input(shape=(784,))

# Encoded representation
encoded = Dense(encoding_dim, activation='relu')(input_img)

# Decoded representation
decoded = Dense(784, activation='sigmoid')(encoded)

# Autoencoder model
autoencoder = Model(input_img, decoded)

# Encoder model
encoder = Model(input_img, encoded)

# Compile the autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Train the autoencoder
autoencoder_history = autoencoder.fit(
    X_train, X_train,
    epochs=50,
    batch_size=256,
    shuffle=True,
    validation_data=(X_test, X_test)
)

# ==========================
# Encode the Images
# ==========================

encoded_X_train = encoder.predict(X_train)
encoded_X_test = encoder.predict(X_test)

# ==========================
# Build and Train Classifier
# ==========================

# Build a classifier model
classifier = Sequential()
classifier.add(Dense(128, activation='relu', input_shape=(encoding_dim,)))
classifier.add(Dropout(0.5))
classifier.add(Dense(10, activation='softmax'))

# Compile the classifier
classifier.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the classifier
classifier_history = classifier.fit(
    encoded_X_train, y_train,
    epochs=50,
    batch_size=256,
    shuffle=True,
    validation_data=(encoded_X_test, y_test)
)

# ==========================
# Evaluate the Classifier
# ==========================

test_loss, test_acc = classifier.evaluate(encoded_X_test, y_test)
print(f'Test accuracy: {test_acc:.4f}')

# ==========================
# Plotting the Results
# ==========================

# Function to plot training history
def plot_history(history, title):
    plt.figure(figsize=(12, 5))

    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history.get('val_loss'), label='Validation Loss')
    plt.title(f'{title} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot accuracy if available
    if 'accuracy' in history.history or 'acc' in history.history:
        plt.subplot(1, 2, 2)
        # Handle different key names for accuracy
        acc = history.history.get('accuracy') or history.history.get('acc')
        val_acc = history.history.get('val_accuracy') or history.history.get('val_acc')
        if acc is not None and val_acc is not None:
            plt.plot(acc, label='Train Accuracy')
            plt.plot(val_acc, label='Validation Accuracy')
            plt.title(f'{title} Accuracy')
            plt.xlabel('Epochs')
            plt.ylabel('Accuracy')
            plt.legend()

    plt.tight_layout()
    plt.show()

# Plot Autoencoder Loss
plot_history(autoencoder_history, 'Autoencoder')

# Plot Classifier Loss and Accuracy
plot_history(classifier_history, 'Classifier')
