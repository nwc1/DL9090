# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# Set hyperparameters
batch_size = 256
num_epochs = 5
learning_rate = 0.001
num_visible = 784
num_hidden = 512

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load MNIST dataset
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Data loader
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Define RBM model
class RBM(nn.Module):
    def __init__(self, num_visible, num_hidden):
        super(RBM, self).__init__()
        self.W = nn.Parameter(torch.randn(num_visible, num_hidden) * 0.1)
        self.v_bias = nn.Parameter(torch.zeros(num_visible))
        self.h_bias = nn.Parameter(torch.zeros(num_hidden))

    def sample_h(self, x):
        wx = torch.matmul(x, self.W) + self.h_bias
        return torch.sigmoid(wx)

    def sample_v(self, h):
        wh = torch.matmul(h, self.W.t()) + self.v_bias
        return torch.sigmoid(wh)

    def forward(self, x):
        h = self.sample_h(x)
        return h

# Initialize RBM model, loss function, and optimizer
model = RBM(num_visible, num_hidden).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    for i, (x, _) in enumerate(train_loader):
        x = x.view(-1, 28*28).to(device)
        x = x.bernoulli()  # Convert to binary

        # Contrastive Divergence (CD) algorithm
        h = model.sample_h(x)
        x_recon = model.sample_v(h)
        h_recon = model.sample_h(x_recon)

        # Calculate loss
        loss = criterion(x, x_recon)

        # Backpropagate and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Print loss at each 100 mini-batches
        if i % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')

# Generate new images
model.eval()
with torch.no_grad():
    test_images = next(iter(test_loader))[0].view(-1, 28*28).to(device)
    test_images = test_images.bernoulli()
    generated_images = model.sample_v(torch.sigmoid(torch.matmul(test_images, model.W) + model.h_bias))

# Visualize real and generated images
num_images = 3
fig, axes = plt.subplots(nrows=2, ncols=num_images, figsize=(15, 6))

for i in range(num_images):
    # Plot real images
    axes[0, i].imshow(test_images[i].reshape(28, 28), cmap='gray')
    axes[0, i].set_title(f'Real Image {i+1}')
    axes[0, i].axis('off')

    # Plot generated images
    axes[1, i].imshow(generated_images[i].reshape(28, 28), cmap='gray')
    axes[1, i].set_title(f'Generated Image {i+1}')
    axes[1, i].axis('off')

plt.tight_layout()
plt.show()