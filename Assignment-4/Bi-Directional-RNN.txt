import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense, Bidirectional
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# Load data
max_words = 1000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)

# Pad sequences
max_length = 300  # Reduced max length for faster training
x_train = pad_sequences(x_train, maxlen=max_length)
x_test = pad_sequences(x_test, maxlen=max_length)

# Function to create a simple model
def create_model(model_type='simple'):
    model = Sequential()
    model.add(Embedding(input_dim=max_words, output_dim=32, input_length=max_length))  # Reduced output_dim
    if model_type == 'simple':
        model.add(SimpleRNN(32))  # Reduced units
    elif model_type == 'bi_rnn':
        model.add(Bidirectional(SimpleRNN(32)))
    elif model_type == 'bi_lstm':
        model.add(Bidirectional(LSTM(32)))
    elif model_type == 'bi_gru':
        model.add(Bidirectional(GRU(32)))

    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

def train_and_evaluate_model(model_fn):
    model = model_fn()
    # Set epochs to 3
    history = model.fit(x_train, y_train, epochs=2, validation_split=0.1, verbose=1)  # Reduced epochs
    loss, accuracy = model.evaluate(x_test, y_test)
    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')
    return model, history

models = {
    "Simple RNN": lambda: create_model('simple'),
    "Bidirectional RNN": lambda: create_model('bi_rnn'),
    "Bidirectional LSTM": lambda: create_model('bi_lstm'),
    "Bidirectional GRU": lambda: create_model('bi_gru'),
}

histories = {}

for model_name, model_fn in models.items():
    print(f"\nTraining {model_name}...")
    model, history = train_and_evaluate_model(model_fn)
    histories[model_name] = history

# Plotting results
plt.figure(figsize=(12, 8))

for model_name, history in histories.items():
    plt.subplot(2, 1, 1)
    plt.plot(history.history['loss'], label=f'{model_name} Training Loss')
    plt.plot(history.history['val_loss'], label=f'{model_name} Validation Loss')

    plt.subplot(2, 1, 2)
    plt.plot(history.history['accuracy'], label=f'{model_name} Training Accuracy')
    plt.plot(history.history['val_accuracy'], label=f'{model_name} Validation Accuracy')

plt.subplot(2, 1, 1)
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.title('Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

def predict_sentiment(model, review):
    word_index = imdb.get_word_index()
    review = review.lower()
    review = ''.join([c for c in review if c.isalnum() or c.isspace()])
    words = review.split()

    encoded_review = [word_index.get(word, 0) for word in words if word_index.get(word, 0) < max_words]
    padded_review = pad_sequences([encoded_review], maxlen=max_length)

    prediction = model.predict(padded_review)
    return "Positive" if prediction[0][0] > 0.5 else "Negative"

sample_reviews = ["I really enjoyed this movie!", "This was the worst film I've ever seen."]
for model_name, model_fn in models.items():
    print(f"\nPredictions using {model_name}:")
    model = model_fn()
    model.fit(x_train, y_train, epochs=1, validation_split=0.1, verbose=0)
    for review in sample_reviews:
        print(f"{review} -> {predict_sentiment(model, review)}")